/*
 * Copyright 2021-2025 the original author or authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *      https://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package dk.trustworks.essentials.components.queue.postgresql;

import dk.trustworks.essentials.components.foundation.IOExceptionUtil;
import dk.trustworks.essentials.components.foundation.json.*;
import dk.trustworks.essentials.components.foundation.messaging.queue.*;
import dk.trustworks.essentials.components.foundation.messaging.queue.operations.*;
import dk.trustworks.essentials.components.foundation.postgresql.*;
import dk.trustworks.essentials.components.foundation.transaction.*;
import dk.trustworks.essentials.components.foundation.transaction.jdbi.*;
import dk.trustworks.essentials.components.queue.postgresql.jdbi.*;
import dk.trustworks.essentials.reactive.*;
import dk.trustworks.essentials.shared.Exceptions;
import dk.trustworks.essentials.shared.collections.Lists;
import org.jdbi.v3.core.Handle;
import org.jdbi.v3.core.mapper.RowMapper;
import org.jdbi.v3.core.statement.StatementContext;
import org.slf4j.*;

import java.sql.*;
import java.time.*;
import java.util.*;
import java.util.concurrent.*;
import java.util.function.*;
import java.util.stream.Collectors;

import static dk.trustworks.essentials.shared.FailFast.*;
import static dk.trustworks.essentials.shared.MessageFormatter.NamedArgumentBinding.arg;
import static dk.trustworks.essentials.shared.MessageFormatter.*;
import static dk.trustworks.essentials.shared.interceptor.DefaultInterceptorChain.sortInterceptorsByOrder;
import static dk.trustworks.essentials.shared.interceptor.InterceptorChain.newInterceptorChainForOperation;

/**
 * Postgresql version of the {@link DurableQueues} concept.<br>
 * Works together with {@link UnitOfWorkFactory} in order to support queuing message together with business logic (such as failing to handle an Event, etc.)<br>
 * <br>
 * Per default this implementation uses the centralized message fetcher, {@link CentralizedMessageFetcher}, to optimize database operations
 * and supports batch fetching of messages across multiple queues.<br>
 * <u>Security</u><br>
 * {@link DurableQueues} allows the user of the component to override the {@link #getSharedQueueTableName()}, which is the name of the table that will contain all messages (across all {@link QueueName}'s)<br>
 * <strong>Note:</strong><br>
 * To support customization of storage table name, the {@code sharedQueueTableName} will be directly used in constructing SQL statements
 * through string concatenation, which exposes the component to SQL injection attacks.<br>
 * <br>
 * <strong>Security Note:</strong><br>
 * <b>It is the responsibility of the user of this component to sanitize the {@code sharedQueueTableName}
 * to ensure the security of all the SQL statements generated by this component.</b><br>
 * The {@link PostgresqlDurableQueues} component will
 * call the {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} method to validate the table name as a first line of defense.<br>
 * The {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} provides an initial layer of defense against SQL injection by applying naming conventions intended to reduce the risk of malicious input.<br>
 * However, Essentials components as well as {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} does not offer exhaustive protection, nor does it assure the complete security of the resulting SQL against SQL injection threats.<br>
 * <b>The responsibility for implementing protective measures against SQL Injection lies exclusively with the users/developers using the Essentials components and its supporting classes.</b><br>
 * Users must ensure thorough sanitization and validation of API input parameters, values, column names, function names, table names, and index names.<br>
 * Insufficient attention to these practices may leave the application vulnerable to SQL injection, potentially endangering the security and integrity of the database.<br>
 * <b>The responsibility for implementing protective measures against SQL Injection lies exclusively with the users/developers using the Essentials components and its supporting classes</b>.<br>
 * Users must ensure thorough sanitization and validation of API input parameters, values, column names, function names, table names, and index names.<br>
 * Insufficient attention to these practices may leave the application vulnerable to SQL injection, potentially endangering the security and integrity of the database.<br>
 * <br>
 * It is highly recommended that the {@code sharedQueueTableName} value is only derived from a controlled and trusted source.<br>
 * To mitigate the risk of SQL injection attacks, external or untrusted inputs should never directly provide the {@code sharedQueueTableName} value.<br>
 */
public final class PostgresqlDurableQueues implements BatchMessageFetchingCapableDurableQueues {
    private static final Logger log                               = LoggerFactory.getLogger(PostgresqlDurableQueues.class);
    public static final  String DEFAULT_DURABLE_QUEUES_TABLE_NAME = "durable_queues";
    private static final Object NO_PAYLOAD                        = new Object();

    private final HandleAwareUnitOfWorkFactory<? extends HandleAwareUnitOfWork>           unitOfWorkFactory;
    private final JSONSerializer                                                          jsonSerializer;
    private final String                                                                  sharedQueueTableName;
    private final ConcurrentMap<QueueName, CentralizedMessageFetcherDurableQueueConsumer> durableQueueConsumers            = new ConcurrentHashMap<>();
    private final ConcurrentMap<QueueName, PostgresqlDurableQueueConsumer>                traditionalDurableQueueConsumers = new ConcurrentHashMap<>();
    private final QueuedMessageRowMapper                                                  queuedMessageMapper;
    private final List<DurableQueuesInterceptor>                                          interceptors                     = new CopyOnWriteArrayList<>();
    private final Optional<MultiTableChangeListener<TableChangeNotification>>             multiTableChangeListener;
    private final Function<ConsumeFromQueue, QueuePollingOptimizer>                       queuePollingOptimizerFactory;
    private final TransactionalMode                                                       transactionalMode;
    private       CentralizedMessageFetcher                                               centralizedMessageFetcher;
    /**
     * Flag indicating whether to use the {@link CentralizedMessageFetcher} or the legacy
     * {@link DefaultDurableQueueConsumer} approach for handling messages
     */
    private final boolean                                                                 useCentralizedMessageFetcher;
    private final boolean                                                                 useOrderedUnorderedQuery;

    private final DurableQueuesSql           durableQueuesSql;
    private final DurableQueuesSerialization durableQueuesSerialization;

    private   Function<QueueName, QueuePollingOptimizer> centralizedQueuePollingOptimizerFactory;
    /**
     * Only used if {@link #transactionalMode} has value {@link TransactionalMode#SingleOperationTransaction}
     */
    private   int                                        messageHandlingTimeoutMs;
    /**
     * Contains the timestamp of the last performed {@link #resetMessagesStuckBeingDelivered(QueueName)} check<br>
     * Only used if {@link #transactionalMode} has value {@link TransactionalMode#SingleOperationTransaction}
     */
    protected ConcurrentMap<QueueName, Instant>          lastResetStuckMessagesCheckTimestamps = new ConcurrentHashMap<>();

    private volatile boolean started;

    public static PostgresqlDurableQueuesBuilder builder() {
        return new PostgresqlDurableQueuesBuilder();
    }

    /**
     * Create {@link DurableQueues} with sharedQueueTableName: {@value DEFAULT_DURABLE_QUEUES_TABLE_NAME} and the default {@link JacksonJSONSerializer} using {@link DurableQueuesSerialization#createDefaultObjectMapper()}
     * configuration<br>
     * Uses the centralized message fetcher with a 20ms polling interval
     *
     * @param unitOfWorkFactory the {@link UnitOfWorkFactory} needed to access the database
     */
    public PostgresqlDurableQueues(HandleAwareUnitOfWorkFactory<? extends HandleAwareUnitOfWork> unitOfWorkFactory) {
        this(unitOfWorkFactory,
             new JacksonJSONSerializer(DurableQueuesSerialization.createDefaultObjectMapper()),
             DEFAULT_DURABLE_QUEUES_TABLE_NAME,
             null,
             null);
    }

    /**
     * Create {@link DurableQueues} with sharedQueueTableName: {@value DEFAULT_DURABLE_QUEUES_TABLE_NAME} and the default {@link JacksonJSONSerializer} using {@link DurableQueuesSerialization#createDefaultObjectMapper()}
     * configuration<br>
     * Uses the centralized message fetcher with a 20ms polling interval
     *
     * @param unitOfWorkFactory            the {@link UnitOfWorkFactory} needed to access the database
     * @param queuePollingOptimizerFactory optional {@link QueuePollingOptimizer} factory that creates a {@link QueuePollingOptimizer} per {@link ConsumeFromQueue} command -
     *                                     if set to null a default {@link SimpleQueuePollingOptimizer} is used instead using, which sets
     *                                     <ul>
     *                                       <li>The minimum polling interval is set to 50% of the consumer's configured polling interval</li>
     *                                       <li>The maximum polling interval is set to 20x the consumer's configured polling interval</li>
     *                                     </ul>
     *                                     This provides an adaptive polling mechanism that reduces database load when queues are empty while
     *                                     maintaining responsiveness when messages arrive.
     */
    public PostgresqlDurableQueues(HandleAwareUnitOfWorkFactory<? extends HandleAwareUnitOfWork> unitOfWorkFactory,
                                   Function<ConsumeFromQueue, QueuePollingOptimizer> queuePollingOptimizerFactory) {
        this(unitOfWorkFactory,
             new JacksonJSONSerializer(DurableQueuesSerialization.createDefaultObjectMapper()),
             DEFAULT_DURABLE_QUEUES_TABLE_NAME,
             null,
             queuePollingOptimizerFactory);
    }

    /**
     * Create {@link DurableQueues} with custom jsonSerializer with sharedQueueTableName: {@value DEFAULT_DURABLE_QUEUES_TABLE_NAME}<br>
     * Uses the centralized message fetcher with a 20ms polling interval
     *
     * @param unitOfWorkFactory the {@link UnitOfWorkFactory} needed to access the database
     * @param jsonSerializer    the {@link JSONSerializer} that is used to serialize/deserialize message payloads
     */
    public PostgresqlDurableQueues(HandleAwareUnitOfWorkFactory<? extends HandleAwareUnitOfWork> unitOfWorkFactory,
                                   JSONSerializer jsonSerializer) {
        this(unitOfWorkFactory,
             jsonSerializer,
             DEFAULT_DURABLE_QUEUES_TABLE_NAME,
             null,
             null);
    }

    /**
     * Create {@link DurableQueues} with custom jsonSerializer with sharedQueueTableName: {@value DEFAULT_DURABLE_QUEUES_TABLE_NAME}<br>
     * Uses the centralized message fetcher with a 20ms polling interval
     *
     * @param unitOfWorkFactory            the {@link UnitOfWorkFactory} needed to access the database
     * @param jsonSerializer               the {@link JSONSerializer} that is used to serialize/deserialize message payloads
     * @param queuePollingOptimizerFactory optional {@link QueuePollingOptimizer} factory that creates a {@link QueuePollingOptimizer} per {@link ConsumeFromQueue} command -
     *                                     if set to null a default {@link SimpleQueuePollingOptimizer} is used instead using, which sets
     *                                     <ul>
     *                                       <li>The minimum polling interval is set to 50% of the consumer's configured polling interval</li>
     *                                       <li>The maximum polling interval is set to 20x the consumer's configured polling interval</li>
     *                                     </ul>
     *                                     This provides an adaptive polling mechanism that reduces database load when queues are empty while
     *                                     maintaining responsiveness when messages arrive.
     */
    public PostgresqlDurableQueues(HandleAwareUnitOfWorkFactory<? extends HandleAwareUnitOfWork> unitOfWorkFactory,
                                   JSONSerializer jsonSerializer,
                                   Function<ConsumeFromQueue, QueuePollingOptimizer> queuePollingOptimizerFactory) {
        this(unitOfWorkFactory,
             jsonSerializer,
             DEFAULT_DURABLE_QUEUES_TABLE_NAME,
             null,
             queuePollingOptimizerFactory);
    }

    /**
     * Create {@link DurableQueues} with custom jsonSerializer and sharedQueueTableName<br>
     * Uses the centralized message fetcher with a 20ms polling interval
     *
     * @param unitOfWorkFactory            the {@link UnitOfWorkFactory} needed to access the database
     * @param jsonSerializer               the {@link JSONSerializer} that is used to serialize/deserialize message payloads
     * @param sharedQueueTableName         the name of the table that will contain all messages (across all {@link QueueName}'s)<br>
     *                                     <strong>Note:</strong><br>
     *                                     To support customization of storage table name, the {@code sharedQueueTableName} will be directly used in constructing SQL statements
     *                                     through string concatenation, which exposes the component to SQL injection attacks.<br>
     *                                     <br>
     *                                     <strong>Security Note:</strong><br>
     *                                     <b>It is the responsibility of the user of this component to sanitize the {@code sharedQueueTableName}
     *                                     to ensure the security of all the SQL statements generated by this component.</b><br>
     *                                     The {@link PostgresqlDurableQueues} component will
     *                                     call the {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} method to validate the table name as a first line of defense.<br>
     *                                     The {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} provides an initial layer of defense against SQL injection by applying naming conventions intended to reduce the risk of malicious input.<br>
     *                                     However, Essentials components as well as {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} does not offer exhaustive protection, nor does it assure the complete security of the resulting SQL against SQL injection threats.<br>
     *                                     <b>The responsibility for implementing protective measures against SQL Injection lies exclusively with the users/developers using the Essentials components and its supporting classes</b>.<br>
     *                                     Users must ensure thorough sanitization and validation of API input parameters, values, column names, function names, table names, and index names.<br>
     *                                     Insufficient attention to these practices may leave the application vulnerable to SQL injection, potentially endangering the security and integrity of the database.<br>
     *                                     <br>
     *                                     It is highly recommended that the {@code sharedQueueTableName} value is only derived from a controlled and trusted source.<br>
     *                                     To mitigate the risk of SQL injection attacks, external or untrusted inputs should never directly provide the {@code sharedQueueTableName} value.<br>
     * @param multiTableChangeListener     optional {@link MultiTableChangeListener} that allows {@link PostgresqlDurableQueues} to use {@link QueuePollingOptimizer}
     * @param queuePollingOptimizerFactory optional {@link QueuePollingOptimizer} factory that creates a {@link QueuePollingOptimizer} per {@link ConsumeFromQueue} command -
     *                                     if set to null a default {@link SimpleQueuePollingOptimizer} is used instead using, which sets
     *                                     <ul>
     *                                       <li>The minimum polling interval is set to 50% of the consumer's configured polling interval</li>
     *                                       <li>The maximum polling interval is set to 20x the consumer's configured polling interval</li>
     *                                     </ul>
     *                                     This provides an adaptive polling mechanism that reduces database load when queues are empty while
     *                                     maintaining responsiveness when messages arrive.
     */
    public PostgresqlDurableQueues(HandleAwareUnitOfWorkFactory<? extends HandleAwareUnitOfWork> unitOfWorkFactory,
                                   JSONSerializer jsonSerializer,
                                   String sharedQueueTableName,
                                   MultiTableChangeListener<TableChangeNotification> multiTableChangeListener,
                                   Function<ConsumeFromQueue, QueuePollingOptimizer> queuePollingOptimizerFactory) {
        this(unitOfWorkFactory,
             jsonSerializer,
             sharedQueueTableName,
             multiTableChangeListener,
             queuePollingOptimizerFactory,
             TransactionalMode.FullyTransactional,
             null);
    }

    /**
     * Create {@link DurableQueues} with custom jsonSerializer and sharedQueueTableName<br>
     * Uses the centralized message fetcher with a 20ms polling interval
     * <br>
     *
     * @param unitOfWorkFactory            the {@link UnitOfWorkFactory} needed to access the database
     * @param jsonSerializer               the {@link JSONSerializer} that is used to serialize/deserialize message payloads
     * @param sharedQueueTableName         the name of the table that will contain all messages (across all {@link QueueName}'s)<br>
     *                                     <strong>Note:</strong><br>
     *                                     To support customization of storage table name, the {@code sharedQueueTableName} will be directly used in constructing SQL statements
     *                                     through string concatenation, which exposes the component to SQL injection attacks.<br>
     *                                     <br>
     *                                     <strong>Security Note:</strong><br>
     *                                     <b>It is the responsibility of the user of this component to sanitize the {@code sharedQueueTableName}
     *                                     to ensure the security of all the SQL statements generated by this component.</b><br>
     *                                     The {@link PostgresqlDurableQueues} component will
     *                                     call the {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} method to validate the table name as a first line of defense.<br>
     *                                     The {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} provides an initial layer of defense against SQL injection by applying naming conventions intended to reduce the risk of malicious input.<br>
     *                                     However, Essentials components as well as {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} does not offer exhaustive protection, nor does it assure the complete security of the resulting SQL against SQL injection threats.<br>
     *                                     <b>The responsibility for implementing protective measures against SQL Injection lies exclusively with the users/developers using the Essentials components and its supporting classes</b>.<br>
     *                                     Users must ensure thorough sanitization and validation of API input parameters, values, column names, function names, table names, and index names.<br>
     *                                     Insufficient attention to these practices may leave the application vulnerable to SQL injection, potentially endangering the security and integrity of the database.<br>
     *                                     <br>
     *                                     It is highly recommended that the {@code sharedQueueTableName} value is only derived from a controlled and trusted source.<br>
     *                                     To mitigate the risk of SQL injection attacks, external or untrusted inputs should never directly provide the {@code sharedQueueTableName} value.<br>
     * @param multiTableChangeListener     optional {@link MultiTableChangeListener} that allows {@link PostgresqlDurableQueues} to use {@link QueuePollingOptimizer}
     * @param queuePollingOptimizerFactory optional {@link QueuePollingOptimizer} factory that creates a {@link QueuePollingOptimizer} per {@link ConsumeFromQueue} command -
     *                                     if set to null a default {@link SimpleQueuePollingOptimizer} is used instead using, which sets
     *                                     <ul>
     *                                       <li>The minimum polling interval is set to 50% of the consumer's configured polling interval</li>
     *                                       <li>The maximum polling interval is set to 20x the consumer's configured polling interval</li>
     *                                     </ul>
     *                                     This provides an adaptive polling mechanism that reduces database load when queues are empty while
     *                                     maintaining responsiveness when messages arrive.
     * @param transactionalMode            The {@link TransactionalMode} for this {@link DurableQueues} instance. If set to {@link TransactionalMode#SingleOperationTransaction}
     *                                     then the consumer MUST call the {@link DurableQueues#acknowledgeMessageAsHandled(AcknowledgeMessageAsHandled)} explicitly in a new {@link UnitOfWork}
     * @param messageHandlingTimeout       Only required if <code>transactionalMode</code> is {@link TransactionalMode#SingleOperationTransaction}.<br>
     *                                     The parameter defines the timeout for messages being delivered, but haven't yet been acknowledged.
     *                                     After this timeout the message delivery will be reset and the message will again be a candidate for delivery
     */
    public PostgresqlDurableQueues(HandleAwareUnitOfWorkFactory<? extends HandleAwareUnitOfWork> unitOfWorkFactory,
                                   JSONSerializer jsonSerializer,
                                   String sharedQueueTableName,
                                   MultiTableChangeListener<TableChangeNotification> multiTableChangeListener,
                                   Function<ConsumeFromQueue, QueuePollingOptimizer> queuePollingOptimizerFactory,
                                   TransactionalMode transactionalMode,
                                   Duration messageHandlingTimeout) {
        this(unitOfWorkFactory,
             jsonSerializer,
             sharedQueueTableName,
             multiTableChangeListener,
             queuePollingOptimizerFactory,
             transactionalMode,
             messageHandlingTimeout,
             true,  // Use centralized message fetcher by default
             Duration.ofMillis(20), // With a 20ms polling interval by default
             null,
             true);
    }

    /**
     * Create {@link DurableQueues} with custom jsonSerializer and sharedQueueTableName
     * <br>
     *
     * @param unitOfWorkFactory                        the {@link UnitOfWorkFactory} needed to access the database
     * @param jsonSerializer                           the {@link JSONSerializer} that is used to serialize/deserialize message payloads
     * @param sharedQueueTableName                     the name of the table that will contain all messages (across all {@link QueueName}'s)<br>
     *                                                 <strong>Note:</strong><br>
     *                                                 To support customization of storage table name, the {@code sharedQueueTableName} will be directly used in constructing SQL statements
     *                                                 through string concatenation, which exposes the component to SQL injection attacks.<br>
     *                                                 <br>
     *                                                 <strong>Security Note:</strong><br>
     *                                                 <b>It is the responsibility of the user of this component to sanitize the {@code sharedQueueTableName}
     *                                                 to ensure the security of all the SQL statements generated by this component.</b><br>
     *                                                 The {@link PostgresqlDurableQueues} component will
     *                                                 call the {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} method to validate the table name as a first line of defense.<br>
     *                                                 The {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} provides an initial layer of defense against SQL injection by applying naming conventions intended to reduce the risk of malicious input.<br>
     *                                                 However, Essentials components as well as {@link PostgresqlUtil#checkIsValidTableOrColumnName(String)} does not offer exhaustive protection, nor does it assure the complete security of the resulting SQL against SQL injection threats.<br>
     *                                                 <b>The responsibility for implementing protective measures against SQL Injection lies exclusively with the users/developers using the Essentials components and its supporting classes</b>.<br>
     *                                                 Users must ensure thorough sanitization and validation of API input parameters, values, column names, function names, table names, and index names.<br>
     *                                                 Insufficient attention to these practices may leave the application vulnerable to SQL injection, potentially endangering the security and integrity of the database.<br>
     *                                                 <br>
     *                                                 It is highly recommended that the {@code sharedQueueTableName} value is only derived from a controlled and trusted source.<br>
     *                                                 To mitigate the risk of SQL injection attacks, external or untrusted inputs should never directly provide the {@code sharedQueueTableName} value.<br>
     * @param multiTableChangeListener                 optional {@link MultiTableChangeListener} that allows {@link PostgresqlDurableQueues} to use {@link QueuePollingOptimizer}
     * @param queuePollingOptimizerFactory             optional {@link QueuePollingOptimizer} factory that creates a {@link QueuePollingOptimizer} per {@link ConsumeFromQueue} command -
     *                                                 if set to null a default {@link SimpleQueuePollingOptimizer} is used instead using, which sets
     *                                                 <ul>
     *                                                   <li>The minimum polling interval is set to 50% of the consumer's configured polling interval</li>
     *                                                   <li>The maximum polling interval is set to 20x the consumer's configured polling interval</li>
     *                                                 </ul>
     *                                                 This provides an adaptive polling mechanism that reduces database load when queues are empty while
     *                                                 maintaining responsiveness when messages arrive.
     * @param transactionalMode                        The {@link TransactionalMode} for this {@link DurableQueues} instance. If set to {@link TransactionalMode#SingleOperationTransaction}
     *                                                 then the consumer MUST call the {@link DurableQueues#acknowledgeMessageAsHandled(AcknowledgeMessageAsHandled)} explicitly in a new {@link UnitOfWork}
     * @param messageHandlingTimeout                   Only required if <code>transactionalMode</code> is {@link TransactionalMode#SingleOperationTransaction}.<br>
     *                                                 The parameter defines the timeout for messages being delivered, but haven't yet been acknowledged.
     *                                                 After this timeout the message delivery will be reset and the message will again be a candidate for delivery
     * @param useCentralizedMessageFetcher             Whether to use the {@link CentralizedMessageFetcher} (true) or fallback to the traditional {@link DefaultDurableQueueConsumer} (false).
     *                                                 The centralized fetcher optimizes message fetching across multiple queues.
     * @param centralizedMessageFetcherPollingInterval Set the polling interval for the {@link CentralizedMessageFetcher}.
     *                                                 This value determines how frequently the fetcher checks for new messages when none are immediately available.
     * @param centralizedQueuePollingOptimizerFactory  Optional factory function that creates a {@link QueuePollingOptimizer} for each queue when using centralized message fetching.
     *                                                 If not provided, a default {@link CentralizedQueuePollingOptimizer} will be used with:
     *                                                 <ul>
     *                                                   <li>Minimum polling interval: 50% of the centralized fetcher's polling interval</li>
     *                                                   <li>Maximum polling interval: 20x the centralized fetcher's polling interval</li>
     *                                                   <li>Polling interval increase factor: 1.5 (50% increase per empty poll)</li>
     *                                                   <li>Polling interval decrease factor: 0.1 (90% decrease when messages found)</li>
     *                                                 </ul>
     * @param useOrderedUnorderedQuery                 a boolean flag that determines whether to use the ordered/unordered query optimization for message fetching. When {@code true}, enables a specialized query strategy that can improve
     *                                                 performance for mixed, ordered and unordered message processing scenarios
     */
    public PostgresqlDurableQueues(HandleAwareUnitOfWorkFactory<? extends HandleAwareUnitOfWork> unitOfWorkFactory,
                                   JSONSerializer jsonSerializer,
                                   String sharedQueueTableName,
                                   MultiTableChangeListener<TableChangeNotification> multiTableChangeListener,
                                   Function<ConsumeFromQueue, QueuePollingOptimizer> queuePollingOptimizerFactory,
                                   TransactionalMode transactionalMode,
                                   Duration messageHandlingTimeout,
                                   boolean useCentralizedMessageFetcher,
                                   Duration centralizedMessageFetcherPollingInterval,
                                   Function<QueueName, QueuePollingOptimizer> centralizedQueuePollingOptimizerFactory,
                                   boolean useOrderedUnorderedQuery) {
        this.unitOfWorkFactory = requireNonNull(unitOfWorkFactory, "No unitOfWorkFactory instance provided");
        this.jsonSerializer = requireNonNull(jsonSerializer, "No jsonSerializer");
        this.sharedQueueTableName = requireNonNull(sharedQueueTableName, "No sharedQueueTableName provided").toLowerCase(Locale.ROOT);
        PostgresqlUtil.checkIsValidTableOrColumnName(sharedQueueTableName);
        this.useCentralizedMessageFetcher = useCentralizedMessageFetcher;
        this.useOrderedUnorderedQuery = useOrderedUnorderedQuery;

        // Initialize helper classes
        this.durableQueuesSql = new DurableQueuesSql(sharedQueueTableName);
        this.durableQueuesSerialization = new DurableQueuesSerialization(jsonSerializer);

        // Create the message row mapper for mapping SQL results to QueuedMessage objects
        this.queuedMessageMapper = new QueuedMessageRowMapper(
                durableQueuesSerialization::deserializeMessagePayload,
                durableQueuesSerialization::deserializeMessageMetadata
        );

        this.multiTableChangeListener = Optional.ofNullable(multiTableChangeListener);
        this.queuePollingOptimizerFactory = queuePollingOptimizerFactory != null ? queuePollingOptimizerFactory : this::createQueuePollingOptimizerFor;
        this.transactionalMode = requireNonNull(transactionalMode, "No transactionalMode instance provided");

        // Initialize the centralized message fetcher
        if (useCentralizedMessageFetcher) {
            this.centralizedMessageFetcher = new CentralizedMessageFetcher(this,
                                                                           requireNonNull(centralizedMessageFetcherPollingInterval, "No centralizedMessageFetcherPollingInterval provided").toMillis(),
                                                                           interceptors);
            this.centralizedQueuePollingOptimizerFactory = centralizedQueuePollingOptimizerFactory != null ? centralizedQueuePollingOptimizerFactory : this::createCentralizedQueuePollingOptimizerFor;
        }

        if (transactionalMode == TransactionalMode.SingleOperationTransaction) {
            messageHandlingTimeoutMs = (int) requireNonNull(messageHandlingTimeout, "No messageHandlingTimeout provided").toMillis();
            addInterceptor(new SingleOperationTransactionDurableQueuesInterceptor(unitOfWorkFactory));
        }
        this.multiTableChangeListener.ifPresent(listener -> listener.addDuplicationFilterAsFirst(new QueueNameDuplicationFilter()));

        initializeQueueTables();
    }

    private void initializeQueueTables() {
        PostgresqlUtil.checkIsValidTableOrColumnName(sharedQueueTableName);
        unitOfWorkFactory.usingUnitOfWork(handleAwareUnitOfWork -> {
            handleAwareUnitOfWork.handle().getJdbi().registerArgument(new QueueNameArgumentFactory());
            handleAwareUnitOfWork.handle().getJdbi().registerColumnMapper(new QueueNameColumnMapper());
            handleAwareUnitOfWork.handle().getJdbi().registerArgument(new QueueEntryIdArgumentFactory());
            handleAwareUnitOfWork.handle().getJdbi().registerColumnMapper(new QueueEntryIdColumnMapper());
            handleAwareUnitOfWork.handle().execute(durableQueuesSql.getCreateQueueTableSql()
                                                  );
            log.info("Ensured Durable Queues table '{}' exists", sharedQueueTableName);

            dropIndex("DROP INDEX IF EXISTS idx_{:tableName}_queue_name",
                      handleAwareUnitOfWork.handle());
            dropIndex("DROP INDEX IF EXISTS idx_{:tableName}_next_delivery_ts",
                      handleAwareUnitOfWork.handle());
            dropIndex("DROP INDEX IF EXISTS idx_{:tableName}_is_dead_letter_message",
                      handleAwareUnitOfWork.handle());
            dropIndex("DROP INDEX IF EXISTS idx_{:tableName}_is_being_delivered",
                      handleAwareUnitOfWork.handle());

            createIndex(durableQueuesSql.getCreateOrderedMessageIndexSql(),
                        handleAwareUnitOfWork.handle());
            createIndex(durableQueuesSql.getCreateNextMessageIndexSql(),
                        handleAwareUnitOfWork.handle());
            createIndex(durableQueuesSql.getCreateNextReadyMessageIndexSql(),
                        handleAwareUnitOfWork.handle());
            createIndex(durableQueuesSql.getCreateOrderedMessageReadyIndexSql(),
                        handleAwareUnitOfWork.handle());
            createIndex(durableQueuesSql.getCreateUnorderedMessageReadyIndexSql(),
                        handleAwareUnitOfWork.handle());
            createIndex(durableQueuesSql.getCreateOrderedMessageHeadIndexSql(),
                        handleAwareUnitOfWork.handle());

            multiTableChangeListener.ifPresent(listener -> {
                ListenNotify.addChangeNotificationTriggerToTable(handleAwareUnitOfWork.handle(),
                                                                 sharedQueueTableName,
                                                                 List.of(ListenNotify.SqlOperation.INSERT, ListenNotify.SqlOperation.UPDATE),
                                                                 "id", "queue_name", "added_ts", "next_delivery_ts", "delivery_ts", "is_dead_letter_message", "is_being_delivered");
            });
        });
    }

    private void createIndex(String indexStatement, Handle handle) {
        PostgresqlUtil.checkIsValidTableOrColumnName(sharedQueueTableName);
        handle.execute(bind(indexStatement,
                            arg("tableName", sharedQueueTableName))
                      );
    }

    private void dropIndex(String indexStatement, Handle handle) {
        PostgresqlUtil.checkIsValidTableOrColumnName(sharedQueueTableName);
        handle.execute(bind(indexStatement,
                            arg("tableName", sharedQueueTableName))
                      );
    }

    /**
     * The name of the shared table where all queue messages are stored
     *
     * @return the name of the shared table where all queue messages are stored
     */
    public String getSharedQueueTableName() {
        return sharedQueueTableName;
    }

    /**
     * Access to the {@link DurableQueuesSql} required for some Integration Tests
     *
     * @return Access to the {@link DurableQueuesSql}
     */
    DurableQueuesSql getDurableQueuesSql() {
        return durableQueuesSql;
    }

    /**
     * Access to the configured {@link DurableQueuesInterceptor}'s
     *
     * @return Read-Only list of the configured {@link DurableQueuesInterceptor}'s
     */
    public List<DurableQueuesInterceptor> getInterceptors() {
        return Collections.unmodifiableList(interceptors);
    }

    @Override
    public void start() {
        if (!started) {
            started = true;
            log.info("Starting PostgresqlDurableQueues");
            PostgresqlUtil.checkIsValidTableOrColumnName(sharedQueueTableName);
            interceptors.forEach(durableQueuesInterceptor -> durableQueuesInterceptor.setDurableQueues(this));
            sortInterceptorsByOrder(interceptors);

            if (useCentralizedMessageFetcher) {
                // Start the centralized message fetcher first
                centralizedMessageFetcher.start();

                // Then start all centralized consumers
                durableQueueConsumers.values().forEach(CentralizedMessageFetcherDurableQueueConsumer::start);
            } else {
                // Start all traditional consumers
                traditionalDurableQueueConsumers.values().forEach(PostgresqlDurableQueueConsumer::start);
            }

            multiTableChangeListener.ifPresent(listener -> {
                listener.listenToNotificationsFor(sharedQueueTableName,
                                                  QueueTableNotification.class);
                listener.getEventBus().addAsyncSubscriber(new AnnotatedEventHandler() {
                    @Handler
                    void handle(QueueTableNotification e) {
                        try {
                            log.trace("[{}] Received Message-Added {} with id '{}'",
                                      e.queueName,
                                      e.getClass().getSimpleName(),
                                      e.id);
                            var queueName = QueueName.of(e.queueName);

                            if (useCentralizedMessageFetcher) {
                                durableQueueConsumers.values()
                                                     .stream()
                                                     .filter(durableQueueConsumer -> durableQueueConsumer.queueName().equals(queueName))
                                                     .forEach(durableQueueConsumer -> {
                                                         // Create a stub QueuedMessage just for notification purposes
                                                         var queuedMessage = createDefaultQueuedMessage(e, queueName);
                                                         // Use the notification interface to optimize polling
                                                         durableQueueConsumer.messageAdded(queuedMessage);
                                                     });
                            } else {
                                // For traditional consumers, pass the notification to the consumer to optimize polling
                                traditionalDurableQueueConsumers.values()
                                                                .stream()
                                                                .filter(durableQueueConsumer -> durableQueueConsumer.queueName().equals(queueName))
                                                                .forEach(durableQueueConsumer -> {
                                                                    // Create a stub QueuedMessage just for notification purposes
                                                                    var queuedMessage = createDefaultQueuedMessage(e, queueName);
                                                                    // Use the notification interface to optimize polling
                                                                    durableQueueConsumer.messageAdded(queuedMessage);
                                                                });
                            }
                        } catch (Exception ex) {
                            log.error("Error occurred while handling notification", ex);
                        }
                    }
                });
            });

            log.info("Started PostgresqlDurableQueues");
        }
    }

    private static DefaultQueuedMessage createDefaultQueuedMessage(QueueTableNotification e, QueueName queueName) {
        return new DefaultQueuedMessage(QueueEntryId.of(String.valueOf(e.id)),
                                        queueName,
                                        Message.of(NO_PAYLOAD),
                                        e.addedTimestamp,
                                        e.nextDeliveryTimestamp,
                                        e.deliveryTimestamp,
                                        null,
                                        -1,
                                        -1,
                                        e.isDeadLetterMessage,
                                        e.isBeingDelivered);
    }

    @Override
    public void stop() {
        if (started) {
            log.info("Stopping");
            PostgresqlUtil.checkIsValidTableOrColumnName(sharedQueueTableName);

            if (useCentralizedMessageFetcher) {
                // Stop all centralized consumer instances first
                durableQueueConsumers.values().forEach(consumer -> {
                    try {
                        consumer.stop();
                    } catch (Exception ex) {
                        if (IOExceptionUtil.isIOException(ex)) {
                            log.debug("Error occurred while stopping CentralizedMessageFetcherDurableQueueConsumer", ex);
                        } else {
                            log.error("Error occurred while stopping CentralizedMessageFetcherDurableQueueConsumer", ex);
                        }
                    }
                });

                // Stop the centralized message fetcher
                try {
                    centralizedMessageFetcher.stop();
                } catch (Exception ex) {
                    if (IOExceptionUtil.isIOException(ex)) {
                        log.debug("Error occurred while stopping CentralizedMessageFetcher", ex);
                    } else {
                        log.error("Error occurred while stopping CentralizedMessageFetcher", ex);
                    }
                }
            } else {
                // Stop all traditional consumer instances
                traditionalDurableQueueConsumers.values().forEach(consumer -> {
                    try {
                        consumer.stop();
                    } catch (Exception ex) {
                        if (IOExceptionUtil.isIOException(ex)) {
                            log.debug("Error occurred while stopping PostgresqlDurableQueueConsumer", ex);
                        } else {
                            log.error("Error occurred while stopping PostgresqlDurableQueueConsumer", ex);
                        }
                    }
                });
            }

            multiTableChangeListener.ifPresent(listener -> {
                try {
                    listener.unlistenToNotificationsFor(sharedQueueTableName);
                } catch (Exception ex) {
                    if (IOExceptionUtil.isIOException(ex)) {
                        log.debug("Error occurred while performing unlistenToNotificationsFor '{}'", sharedQueueTableName, ex);
                    } else {
                        log.error("Error occurred while performing unlistenToNotificationsFor '{}'", sharedQueueTableName, ex);
                    }
                }
            });
            started = false;
            log.info("Stopped");
        }
    }

    @Override
    public boolean isStarted() {
        return started;
    }

    @Override
    public TransactionalMode getTransactionalMode() {
        return transactionalMode;
    }

    @Override
    public Optional<UnitOfWorkFactory<? extends UnitOfWork>> getUnitOfWorkFactory() {
        return Optional.ofNullable(unitOfWorkFactory);
    }

    /**
     * Access to the {@link QueuedMessageRowMapper} required for certain integration tests
     *
     * @return the configured {@link QueuedMessageRowMapper}
     */
    QueuedMessageRowMapper getQueuedMessageMapper() {
        return queuedMessageMapper;
    }

    @Override
    public Set<QueueName> getQueueNames() {
        var consumerQueueNames = getActiveQueueNames();
        var dbQueueNames = unitOfWorkFactory.withUnitOfWork(handleAwareUnitOfWork -> handleAwareUnitOfWork.handle()
                                                                                                          .createQuery(durableQueuesSql.getQueueNamesSql())
                                                                                                          .mapTo(QueueName.class)
                                                                                                          .set());
        dbQueueNames.addAll(consumerQueueNames);
        return dbQueueNames;
    }

    @Override
    public Set<QueueName> getActiveQueueNames() {
        // Combine queue names from both types of consumers
        Set<QueueName> allActiveQueueNames = new HashSet<>();
        allActiveQueueNames.addAll(durableQueueConsumers.keySet());
        allActiveQueueNames.addAll(traditionalDurableQueueConsumers.keySet());
        return allActiveQueueNames;
    }

    @Override
    public DurableQueueConsumer consumeFromQueue(ConsumeFromQueue operation) {
        requireNonNull(operation, "No operation provided");

        // Check if we already have a consumer for this queue
        if (durableQueueConsumers.containsKey(operation.queueName) ||
                traditionalDurableQueueConsumers.containsKey(operation.queueName)) {
            throw new DurableQueueException("There is already a DurableConsumer for this queue", operation.queueName);
        }

        operation.validate();

        if (useCentralizedMessageFetcher) {
            // Create and register a CentralizedMessageFetcherDurableQueueConsumer
            var queuePollingOptimizer = multiTableChangeListener.map(_ignore -> centralizedQueuePollingOptimizerFactory.apply(operation.getQueueName()))
                                                                .orElseGet(QueuePollingOptimizer::None);
            return durableQueueConsumers.computeIfAbsent(operation.queueName, _queueName -> {
                var consumer = (CentralizedMessageFetcherDurableQueueConsumer) newInterceptorChainForOperation(operation,
                                                                                                               interceptors,
                                                                                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                                                                                               () -> (DurableQueueConsumer) new CentralizedMessageFetcherDurableQueueConsumer(
                                                                                                                       operation,
                                                                                                                       this,
                                                                                                                       this::removeQueueConsumer,
                                                                                                                       centralizedMessageFetcher,
                                                                                                                       queuePollingOptimizer)).proceed();
                if (started) {
                    consumer.start();
                }
                log.info("[{}] {} - {} {}",
                         operation.queueName,
                         operation.consumerName,
                         started ? "Started" : "Created",
                         consumer.getClass().getSimpleName()
                        );
                return consumer;
            });
        } else {
            // Create and register a traditional PostgresqlDurableQueueConsumer
            return traditionalDurableQueueConsumers.computeIfAbsent(operation.queueName, _queueName -> {
                var pollingIntervalMs = operation.getPollingInterval().toMillis();
                var queuePollingOptimizer = multiTableChangeListener.map(_ignore -> queuePollingOptimizerFactory.apply(operation))
                                                                    .orElseGet(QueuePollingOptimizer::None);

                var consumer = (PostgresqlDurableQueueConsumer) newInterceptorChainForOperation(operation,
                                                                                                interceptors,
                                                                                                (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                                                                                () -> (DurableQueueConsumer) new PostgresqlDurableQueueConsumer(
                                                                                                        operation,
                                                                                                        unitOfWorkFactory,
                                                                                                        this,
                                                                                                        this::removeQueueConsumer,
                                                                                                        pollingIntervalMs,
                                                                                                        queuePollingOptimizer,
                                                                                                        interceptors)).proceed();
                if (started) {
                    consumer.start();
                }
                log.info("[{}] {} - {} {}",
                         operation.queueName,
                         operation.consumerName,
                         started ? "Started" : "Created",
                         consumer.getClass().getSimpleName()
                        );
                return consumer;
            });
        }
    }

    /**
     * Returns a default {@link SimpleQueuePollingOptimizer} for a given queue when no <code>Function<ConsumeFromQueue, QueuePollingOptimizer> queuePollingOptimizerFactory</code> is provided through the constructor.<br>
     * The optimizer provides an adaptive polling mechanism that reduces database load when queues are empty while
     * maintaining responsiveness when messages arrive.
     *
     * @param operation the {@link ConsumeFromQueue} operation containing the queue consumer's configuration
     * @return a {@link SimpleQueuePollingOptimizer} instance configured with:
     * <ul>
     *   <li>The minimum polling interval is set to 50% of the consumer's configured polling interval</li>
     *   <li>The maximum polling interval is set to 20x the consumer's configured polling interval</li>
     * </ul>
     * @see SimpleQueuePollingOptimizer
     * @see PostgresqlDurableQueues#PostgresqlDurableQueues(HandleAwareUnitOfWorkFactory, JSONSerializer, String, MultiTableChangeListener, Function, TransactionalMode, Duration)
     */
    private QueuePollingOptimizer createQueuePollingOptimizerFor(ConsumeFromQueue operation) {
        var pollingIntervalMs = operation.getPollingInterval().toMillis();
        return new SimpleQueuePollingOptimizer(operation,
                                               (long) (pollingIntervalMs * 0.5d),
                                               pollingIntervalMs * 20);
    }

    /**
     * Creates a default {@link CentralizedQueuePollingOptimizer} for a given queue when using the centralized message fetcher and when no
     * <code>Function<ConsumeFromQueue, QueuePollingOptimizer> centralizedQueuePollingOptimizerFactory</code> is provided through the constructor.
     * The optimizer provides an adaptive polling mechanism that reduces database load when queues are empty while
     * maintaining responsiveness when messages arrive.
     *
     * @param queueName the queue name for which to create the polling optimizer
     * @return a new {@link CentralizedQueuePollingOptimizer} configured with:
     * <ul>
     *   <li>Minimum polling interval: 50% of the centralized fetcher's polling interval</li>
     *   <li>Maximum polling interval: 20x the centralized fetcher's polling interval</li>
     *   <li>Polling interval increase factor: 1.5 (50% increase per empty poll)</li>
     *   <li>Polling interval decrease factor: 0.1 (90% decrease when messages found)</li>
     * </ul>
     * @throws IllegalStateException if called and the centralized message fetcher is not enabled
     * @see PostgresqlDurableQueues#PostgresqlDurableQueues(HandleAwareUnitOfWorkFactory, JSONSerializer, String, MultiTableChangeListener, Function, TransactionalMode, Duration, boolean, Duration, Function, boolean)
     */
    private QueuePollingOptimizer createCentralizedQueuePollingOptimizerFor(QueueName queueName) {
        if (!useCentralizedMessageFetcher) {
            throw new IllegalStateException("The CentralizedMessageFetcher is not enabled, so a custom QueuePollingOptimizer must be provided");
        }
        var pollingIntervalMs = centralizedMessageFetcher.getPollingIntervalMs();
        return new CentralizedQueuePollingOptimizer(queueName,
                                                    (long) (pollingIntervalMs * 0.5d),
                                                    pollingIntervalMs * 20,
                                                    1.5,
                                                    0.1
        );
    }

    protected void removeQueueConsumer(DurableQueueConsumer durableQueueConsumer) {
        requireNonNull(durableQueueConsumer, "You must provide a durableQueueConsumer");
        requireFalse(durableQueueConsumer.isStarted(), msg("Cannot remove DurableQueueConsumer '{}' since it's started!", durableQueueConsumer.queueName()));
        var operation = new StopConsumingFromQueue(durableQueueConsumer);
        try {
            newInterceptorChainForOperation(operation,
                                            interceptors,
                                            (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                            () -> {
                                                var queueName = durableQueueConsumer.queueName();

                                                if (durableQueueConsumer instanceof CentralizedMessageFetcherDurableQueueConsumer) {
                                                    // Unregister from the centralized message fetcher
                                                    centralizedMessageFetcher.unregisterConsumer(queueName);

                                                    // Remove from the centralized consumers map
                                                    return (DurableQueueConsumer) durableQueueConsumers.remove(queueName);
                                                } else {
                                                    // This is a traditional consumer, remove from that map
                                                    return (DurableQueueConsumer) traditionalDurableQueueConsumers.remove(queueName);
                                                }
                                            })
                    .proceed();
        } catch (Exception e) {
            log.error(msg("Failed to perform {}", operation), e);
        }
    }

    @Override
    public QueueEntryId queueMessage(QueueMessage operation) {
        requireNonNull(operation, "You must provide a QueueMessage instance");
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> queueMessage(operation.queueName,
                                                                  operation.getMessage(),
                                                                  false,
                                                                  operation.getCauseOfEnqueuing(),
                                                                  operation.getDeliveryDelay()))
                .proceed();
    }

    @Override
    public QueueEntryId queueMessageAsDeadLetterMessage(QueueMessageAsDeadLetterMessage operation) {
        requireNonNull(operation, "You must provide a QueueMessageAsDeadLetterMessage instance");
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> queueMessage(operation.queueName,
                                                                  operation.getMessage(),
                                                                  true,
                                                                  Optional.ofNullable(operation.getCauseOfError()),
                                                                  Optional.empty()))
                .proceed();
    }

    protected QueueEntryId queueMessage(QueueName queueName,
                                        Message message,
                                        boolean isDeadLetterMessage,
                                        Optional<Exception> causeOfEnqueuing,
                                        Optional<Duration> deliveryDelay) {
        requireNonNull(queueName, "You must provide a queueName");
        requireNonNull(message, "You must provide a message");
        requireNonNull(causeOfEnqueuing, "You must provide a causeOfEnqueuing option");
        requireNonNull(deliveryDelay, "You must provide a deliveryDelay option");

        var queueEntryId          = QueueEntryId.random();
        var addedTimestamp        = Instant.now();
        var nextDeliveryTimestamp = isDeadLetterMessage ? null : addedTimestamp.plus(deliveryDelay.orElse(Duration.ZERO));

        var isOrderedMessage = message instanceof OrderedMessage;
        log.trace("[{}:{}] Queuing {}{}message{} with nextDeliveryTimestamp {}",
                  queueName,
                  queueEntryId,
                  isDeadLetterMessage ? "Dead Letter " : "",
                  isOrderedMessage ? "Ordered " : "",
                  isOrderedMessage ? msg(" {}:{}", ((OrderedMessage) message).getKey(), ((OrderedMessage) message).getOrder()) : "",
                  nextDeliveryTimestamp);

        String jsonPayload;
        try {
            jsonPayload = jsonSerializer.serialize(message.getPayload());
        } catch (JSONSerializationException e) {
            throw new DurableQueueException(msg("Failed to serialize message payload of type", message.getPayload().getClass().getName()), e, queueName);
        }

        if (transactionalMode == TransactionalMode.FullyTransactional) {
            unitOfWorkFactory.getRequiredUnitOfWork();
        }

        unitOfWorkFactory.usingUnitOfWork(unitOfWork -> {
            var update = unitOfWork.handle().createUpdate(durableQueuesSql.getQueueMessageSql())
                                   .bind("id", queueEntryId)
                                   .bind("queueName", queueName)
                                   .bind("message_payload", jsonPayload)
                                   .bind("message_payload_type", message.getPayload().getClass().getName())
                                   .bind("addedTimestamp", addedTimestamp)
                                   .bind("nextDeliveryTimestamp", nextDeliveryTimestamp)
                                   .bind("isDeadLetterMessage", isDeadLetterMessage);

            if (message instanceof OrderedMessage orderedMessage) {
                requireNonNull(orderedMessage.getKey(), "An OrderedMessage requires a non null key");
                requireTrue(orderedMessage.getOrder() >= 0, "An OrderedMessage requires an order >= 0");
                update.bind("deliveryMode", QueuedMessage.DeliveryMode.IN_ORDER)
                      .bind("key", orderedMessage.getKey())
                      .bind("order", orderedMessage.getOrder());
            } else {
                update.bind("deliveryMode", QueuedMessage.DeliveryMode.NORMAL)
                      .bindNull("key", Types.VARCHAR)
                      .bind("order", -1L);

            }

            try {
                var jsonMetaData = jsonSerializer.serialize(message.getMetaData());
                update.bind("metaData", jsonMetaData);
            } catch (JSONSerializationException e) {
                throw new DurableQueueException("Failed to serialize message meta-data", e, queueName);
            }

            if (causeOfEnqueuing.isPresent()) {
                update.bind("lastDeliveryError", causeOfEnqueuing.map(Exceptions::getStackTrace).get());
            } else {
                update.bindNull("lastDeliveryError", Types.VARCHAR);
            }

            var numberOfRowsUpdated = update.execute();
            if (numberOfRowsUpdated == 0) {
                throw new DurableQueueException("Failed to insert message", queueName);
            }
        });
        log.debug("[{}:{}] Queued {}{}message{} with nextDeliveryTimestamp {}",
                  queueName,
                  queueEntryId,
                  isDeadLetterMessage ? "Dead Letter " : "",
                  isOrderedMessage ? "Ordered " : "",
                  isOrderedMessage ? msg(" {}:{}", ((OrderedMessage) message).getKey(), ((OrderedMessage) message).getOrder()) : "",
                  nextDeliveryTimestamp);
        return queueEntryId;
    }

    @Override
    public List<QueueEntryId> queueMessages(QueueMessages operation) {
        requireNonNull(operation, "You must provide a QueueMessages instance");
        operation.validate();

        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> {
                                                   var queueName             = operation.getQueueName();
                                                   var deliveryDelay         = operation.getDeliveryDelay();
                                                   var messages              = operation.getMessages();
                                                   var addedTimestamp        = Instant.now();
                                                   var nextDeliveryTimestamp = addedTimestamp.plus(deliveryDelay.orElse(Duration.ZERO));

                                                   var batch = unitOfWorkFactory.getRequiredUnitOfWork().handle().prepareBatch(durableQueuesSql.getQueueMessageSql());
                                                   var queueEntryIds = Lists.toIndexedStream(messages).map(indexedMessage -> {
                                                       var    message = indexedMessage._2;
                                                       String jsonPayload;
                                                       try {
                                                           jsonPayload = jsonSerializer.serialize(message.getPayload());
                                                       } catch (JSONSerializationException e) {
                                                           throw new DurableQueueException(msg("Failed to serialize message payload of type", message.getPayload().getClass().getName()), e, queueName);
                                                       }
                                                       var queueEntryId = QueueEntryId.random();
                                                       batch.bind("id", queueEntryId)
                                                            .bind("queueName", queueName)
                                                            .bind("message_payload", jsonPayload)
                                                            .bind("message_payload_type", message.getPayload().getClass().getName())
                                                            .bind("addedTimestamp", addedTimestamp)
                                                            .bind("nextDeliveryTimestamp", nextDeliveryTimestamp)
                                                            .bind("isDeadLetterMessage", false)
                                                            .bind("isBeingDelivered", false)
                                                            .bindNull("lastDeliveryError", Types.VARCHAR);

                                                       if (message instanceof OrderedMessage) {
                                                           var orderedMessage = (OrderedMessage) message;
                                                           requireNonNull(orderedMessage.getKey(), msg("[Index: {}] - OrderedMessage requires a non null key", indexedMessage._1));
                                                           requireTrue(orderedMessage.getOrder() >= 0, msg("[Index: {}] - OrderedMessage requires an order >= 0", indexedMessage._1));

                                                           batch.bind("deliveryMode", QueuedMessage.DeliveryMode.IN_ORDER)
                                                                .bind("key", orderedMessage.getKey())
                                                                .bind("order", orderedMessage.getOrder());
                                                       } else {
                                                           batch.bind("deliveryMode", QueuedMessage.DeliveryMode.NORMAL)
                                                                .bindNull("key", Types.VARCHAR)
                                                                .bind("order", -1L);
                                                       }

                                                       try {
                                                           var jsonMetaData = jsonSerializer.serialize(message.getMetaData());
                                                           batch.bind("metaData", jsonMetaData);
                                                       } catch (JSONSerializationException e) {
                                                           throw new DurableQueueException("Failed to serialize message meta-data", e, queueName);
                                                       }

                                                       batch.add();
                                                       return queueEntryId;
                                                   }).collect(Collectors.toList());

                                                   var numberOfRowsUpdated = Arrays.stream(batch.execute())
                                                                                   .reduce(Integer::sum).orElse(0);
                                                   if (numberOfRowsUpdated != messages.size()) {
                                                       throw new DurableQueueException(msg("Attempted to queue {} messages but only inserted {} messages", messages.size(), numberOfRowsUpdated),
                                                                                       queueName);
                                                   }

                                                   log.debug("[{}] Queued {} Messages with nextDeliveryTimestamp {} and entry-id's: {}",
                                                             queueName,
                                                             messages.size(),
                                                             nextDeliveryTimestamp,
                                                             queueEntryIds);
                                                   return queueEntryIds;
                                               }).proceed();
    }

    @Override
    public Optional<QueuedMessage> retryMessage(RetryMessage operation) {
        requireNonNull(operation, "You must provide a RetryMessage instance");
        operation.validate();
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> {
                                                   var nextDeliveryTimestamp = Instant.now().plus(operation.getDeliveryDelay());
                                                   var result = unitOfWorkFactory.getRequiredUnitOfWork().handle().createQuery(durableQueuesSql.getRetryMessageSql())
                                                                                 .bind("nextDeliveryTimestamp", nextDeliveryTimestamp)
                                                                                 .bind("lastDeliveryError", operation.getCauseForRetry() != null ? Exceptions.getStackTrace(operation.getCauseForRetry()) : RetryMessage.MANUALLY_REQUESTED_REDELIVERY)
                                                                                 .bind("id", operation.queueEntryId)
                                                                                 .map(queuedMessageMapper)
                                                                                 .findOne();
                                                   if (result.isPresent()) {
                                                       log.debug("Marked Message with id '{}' for Retry at {}. Message entry after update: {}", operation.queueEntryId, nextDeliveryTimestamp, result.get());
                                                       return result;
                                                   } else {
                                                       log.error("Failed to Mark Message with id '{}' for Retry", operation.queueEntryId);
                                                       return Optional.<QueuedMessage>empty();
                                                   }
                                               }).proceed();
    }


    @Override
    public Optional<QueuedMessage> markAsDeadLetterMessage(MarkAsDeadLetterMessage operation) {
        requireNonNull(operation, "You must provide a MarkAsDeadLetterMessage instance");
        operation.validate();
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> {
                                                   var result = unitOfWorkFactory.getRequiredUnitOfWork().handle().createQuery(durableQueuesSql.getMarkAsDeadLetterMessageSql())
                                                                                 .bind("lastDeliveryError", operation.getCauseForBeingMarkedAsDeadLetter())
                                                                                 .bind("id", operation.queueEntryId)
                                                                                 .map(queuedMessageMapper)
                                                                                 .findOne();
                                                   if (result.isPresent()) {
                                                       log.debug("Marked message with id '{}' as Dead Letter Message. Message entry after update: {}", operation.queueEntryId, result.get());
                                                       return result;
                                                   } else {
                                                       log.error("Failed to Mark as Message message with id '{}' as Dead Letter Message", operation.queueEntryId);
                                                       return Optional.<QueuedMessage>empty();
                                                   }
                                               }).proceed();
    }

    @Override
    public boolean markAsDeadLetterMessageDirect(MarkAsDeadLetterMessageDirect operation) {
        requireNonNull(operation, "You must provide a MarkAsDeadLetterMessageDirect instance");
        operation.validate();
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> {
                                                   var rowsUpdated = unitOfWorkFactory.getRequiredUnitOfWork().handle()
                                                           .createUpdate(durableQueuesSql.getMarkAsDeadLetterMessageDirectSql())
                                                           .bind("lastDeliveryError", operation.getCauseForBeingMarkedAsDeadLetter())
                                                           .bind("id", operation.queueEntryId)
                                                           .execute();
                                                   if (rowsUpdated > 0) {
                                                       log.debug("Marked message with id '{}' as Dead Letter Message (direct, no return)", operation.queueEntryId);
                                                       return true;
                                                   } else {
                                                       log.error("Failed to Mark message with id '{}' as Dead Letter Message (direct)", operation.queueEntryId);
                                                       return false;
                                                   }
                                               }).proceed();
    }

    @Override
    public Optional<QueuedMessage> resurrectDeadLetterMessage(ResurrectDeadLetterMessage operation) {
        requireNonNull(operation, "You must provide a ResurrectDeadLetterMessage instance");
        operation.validate();
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> {
                                                   var nextDeliveryTimestamp = Instant.now().plus(operation.getDeliveryDelay());
                                                   var result = unitOfWorkFactory.getRequiredUnitOfWork().handle().createQuery(durableQueuesSql.getResurrectDeadLetterMessageSql())
                                                                                 .bind("nextDeliveryTimestamp", nextDeliveryTimestamp)
                                                                                 .bind("id", operation.queueEntryId)
                                                                                 .map(queuedMessageMapper)
                                                                                 .findOne();
                                                   if (result.isPresent()) {
                                                       var updateResult = result.get();

                                                       var isOrderedMessage = updateResult.getDeliveryMode() == QueuedMessage.DeliveryMode.IN_ORDER;
                                                       log.debug("[{}] Resurrected Dead Letter {}Message with id '{}' {} and nextDeliveryTimestamp: {}. Message entry after update: {}",
                                                                 updateResult.getQueueName(),
                                                                 isOrderedMessage ? "Ordered " : "",
                                                                 operation.getQueueEntryId(),
                                                                 isOrderedMessage ? "(key: " + ((OrderedMessage) updateResult).getKey() + ", order: " + ((OrderedMessage) updateResult).getOrder() + ")" : "",
                                                                 nextDeliveryTimestamp,
                                                                 updateResult);
                                                       return result;
                                                   } else {
                                                       log.error("Failed to resurrect Dead Letter Message with id '{}'", operation.queueEntryId);
                                                       return Optional.<QueuedMessage>empty();
                                                   }
                                               }).proceed();
    }

    @Override
    public boolean acknowledgeMessageAsHandled(AcknowledgeMessageAsHandled operation) {
        requireNonNull(operation, "You must provide a AcknowledgeMessageAsHandled instance");

        return unitOfWorkFactory.withUnitOfWork(() -> newInterceptorChainForOperation(operation,
                                                                                      interceptors,
                                                                                      (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                                                                      () -> {
                                                                                          log.debug("Acknowledging-Message-As-Handled regarding Message with id '{}'", operation.queueEntryId);
                                                                                          var queueEntryId = operation.queueEntryId;
                                                                                          var rowsUpdated = unitOfWorkFactory.getRequiredUnitOfWork().handle().createUpdate(durableQueuesSql.getAcknowledgeMessageAsHandledSql())
                                                                                                                             .bind("id", operation.queueEntryId)
                                                                                                                             .execute();
                                                                                          if (rowsUpdated == 1) {
                                                                                              log.debug("Acknowledged message as handled and deleted it. Id: '{}'", queueEntryId);
                                                                                              return true;
                                                                                          } else if (getDeadLetterMessage(new GetDeadLetterMessage(operation.queueEntryId)).isPresent()) {
                                                                                              log.debug("Couldn't acknowledge message as it was marked as a Dead-Letter-Message during the message handling. Id: '{}'", queueEntryId);
                                                                                              return true;
                                                                                          } else {
                                                                                              log.error("Couldn't Acknowledge with id '{}' - it may already have been deleted", queueEntryId);
                                                                                              return false;
                                                                                          }
                                                                                      })
                .proceed());

    }

    @Override
    public boolean deleteMessage(DeleteMessage operation) {
        requireNonNull(operation, "You must provide a DeleteMessage instance");

        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> {
                                                   var rowsUpdated = unitOfWorkFactory.getRequiredUnitOfWork().handle().createUpdate(durableQueuesSql.getDeleteMessageSql())
                                                                                      .bind("id", operation.queueEntryId)
                                                                                      .execute();
                                                   if (rowsUpdated == 1) {
                                                       log.debug("Deleted Message with id '{}'", operation.queueEntryId);
                                                       return true;
                                                   } else {
                                                       log.error("Couldn't Delete Message with id '{}' - it may already have been deleted", operation.queueEntryId);
                                                       return false;
                                                   }
                                               }).proceed();
    }

    @Override
    public Optional<QueuedMessage> getNextMessageReadyForDelivery(GetNextMessageReadyForDelivery operation) {
        requireNonNull(operation, "You must provide a GetNextMessageReadyForDelivery instance");
        return getNextMessageReadyForDelivery(operation, useOrderedUnorderedQuery);
    }

    public Optional<QueuedMessage> getNextMessageReadyForDelivery(GetNextMessageReadyForDelivery operation,
                                                                  boolean useOrderedUnorderedQuery) {
        requireNonNull(operation, "You must specify a GetNextMessageReadyForDelivery instance");
        log.trace("[{}] Entered GetNextMessageReadyForDelivery", operation.queueName);

        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptors, interceptorChain) -> interceptors.intercept(operation, interceptorChain),
                                               () -> {
                                                   resetMessagesStuckBeingDelivered(operation.queueName);
                                                   Instant now = Instant.now();
                                                   Collection<String> excludes =
                                                           operation.getExcludeOrderedMessagesWithKey() != null
                                                           ? operation.getExcludeOrderedMessagesWithKey()
                                                           : List.of();

                                                   try {
                                                       if (useOrderedUnorderedQuery) {
                                                           return fetchNextMessageReadyForDeliveryOrderedUnordered(operation.queueName, excludes, now);
                                                       } else {
                                                           return fetchNextMessageReadyForDelivery(operation.queueName, excludes, now);
                                                       }
                                                   } catch (DurableQueueDeserializationException e) {
                                                       log.error("[{}] Marking Message as DeadLetterMessage due to DurableQueueDeserializationException "
                                                                         + "while deserializing message with id '{}'",
                                                                 operation.queueName, e.queueEntryId.get(), e);
                                                       // Use markAsDeadLetterMessageDirect to avoid deserializing the message again
                                                       markAsDeadLetterMessageDirect(e.queueEntryId.get(), e);
                                                       return Optional.<QueuedMessage>empty();
                                                   }
                                               }).proceed();
    }

    private Optional<QueuedMessage> fetchNextMessageReadyForDeliveryOrderedUnordered(QueueName queueName, Collection<String> excludes, Instant now) {
        boolean hasExcludes = !excludes.isEmpty();
        var     orderedSql  = durableQueuesSql.buildOrderedSqlStatement(hasExcludes);
        log.trace("[{}] Executing fetchNextMessageReadyForDeliveryOrderedUnordered sql", queueName);

        var handle = unitOfWorkFactory.getRequiredUnitOfWork().handle();

        var orderedQuery = handle.createQuery(orderedSql)
                                 .bind("queueName", queueName)
                                 .bind("now", now)
                                 .bind("limit", 1);
        if (hasExcludes) orderedQuery.bindList("excludeKeys", excludes);

        Optional<QueuedMessage> queuedMessage = orderedQuery
                .map(queuedMessageMapper)
                .findOne();
        if (queuedMessage.isPresent()) {
            return queuedMessage;
        }

        var unorderedSql = durableQueuesSql.buildUnorderedSqlStatement();

        return handle.createQuery(unorderedSql)
                     .bind("queueName", queueName)
                     .bind("now", now)
                     .bind("limit", 1)
                     .map(queuedMessageMapper)
                     .findOne();
    }


    private Optional<QueuedMessage> fetchNextMessageReadyForDelivery(QueueName queueName, Collection<String> excludes, Instant now) {
        var sql = durableQueuesSql.buildGetNextMessageReadyForDeliverySqlStatement(excludes);
        var query = unitOfWorkFactory
                .getRequiredUnitOfWork()
                .handle()
                .createQuery(sql)
                .bind("queueName", queueName)
                .bind("now", now)
                .bind("limit", 1);

        if (!excludes.isEmpty()) {
            query.bindList("excludedKeys", excludes);
        }
        log.trace("[{}] Executing fetchNextMessageReadyForDelivery sql", queueName);
        return query.map(queuedMessageMapper).findOne();
    }

    /**
     * This operation will scan for messages that has been marked as {@link QueuedMessage#isBeingDelivered()} for longer
     * than {@link #messageHandlingTimeoutMs}<br>
     * All messages found will have {@link QueuedMessage#isBeingDelivered()} and {@link QueuedMessage#getDeliveryTimestamp()}
     * reset<br>
     * Only relevant for when using {@link TransactionalMode#SingleOperationTransaction}
     *
     * @param queueName the queue for which we're looking for messages stuck being marked as {@link QueuedMessage#isBeingDelivered()}
     */
    void resetMessagesStuckBeingDelivered(QueueName queueName) {
        // Reset stuck messages
        if (transactionalMode == TransactionalMode.SingleOperationTransaction) {
            var now                            = Instant.now();
            var lastStuckMessageResetTimestamp = lastResetStuckMessagesCheckTimestamps.get(queueName);
            if (lastStuckMessageResetTimestamp == null || Duration.between(now, lastStuckMessageResetTimestamp).abs().toMillis() > messageHandlingTimeoutMs) {
                if (log.isDebugEnabled()) {
                    log.debug("[{}] Looking for messages stuck marked as isBeingDelivered. Last check was performed: {}", queueName, lastStuckMessageResetTimestamp);
                }

                var numberOfChanges = unitOfWorkFactory.getRequiredUnitOfWork().handle().createUpdate(durableQueuesSql.getResetMessagesStuckBeingDeliveredSql())
                                                       .bind("threshold", now.minusMillis(messageHandlingTimeoutMs))
                                                       .bind("error", "Handler Processing of the Message was determined to have Timed Out")
                                                       .bind("now", now)
                                                       .execute();
                if (numberOfChanges > 0) {
                    log.debug("[{}] Reset {} messages stuck marked as isBeingDelivered", queueName, numberOfChanges);
                } else {
                    log.debug("[{}] Didn't find any messages being stuck marked as isBeingDelivered", queueName);
                }
                lastResetStuckMessagesCheckTimestamps.put(queueName, now);
            }
        }
    }

    @Override
    public boolean hasMessagesQueuedFor(QueueName queueName) {
        return getTotalMessagesQueuedFor(queueName) > 0;
    }


    @Override
    public final boolean hasOrderedMessageQueuedForKey(QueueName queueName, String key) {
        requireNonNull(queueName, "No queueName provided");
        requireNonNull(key, "No key provided");
        return unitOfWorkFactory.withUnitOfWork(handleAwareUnitOfWork -> handleAwareUnitOfWork.handle().createQuery(durableQueuesSql.getHasOrderedMessageQueuedForKeySql())
                                                                                              .bind("queueName", queueName)
                                                                                              .bind("key", key)
                                                                                              .mapTo(Long.class)
                                                                                              .one()) > 0;
    }

    @Override
    public final long getTotalMessagesQueuedFor(GetTotalMessagesQueuedFor operation) {
        requireNonNull(operation, "You must specify a GetTotalMessagesQueuedFor instance");
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> unitOfWorkFactory.withUnitOfWork(handleAwareUnitOfWork -> handleAwareUnitOfWork.handle().createQuery(durableQueuesSql.getGetTotalMessagesQueuedForSql())
                                                                                                                                    .bind("queueName", operation.queueName)
                                                                                                                                    .mapTo(Long.class)
                                                                                                                                    .one()))
                .proceed();
    }

    @Override
    public QueuedMessageCounts getQueuedMessageCountsFor(GetQueuedMessageCountsFor operation) {
        requireNonNull(operation, "You must specify a GetTotalMessagesQueuedFor instance");
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> unitOfWorkFactory.withUnitOfWork(handleAwareUnitOfWork -> handleAwareUnitOfWork.handle().createQuery(durableQueuesSql.getQueuedMessageCountsForSql())
                                                                                                                                    .bind("queueName", operation.queueName)
                                                                                                                                    .map((rs, ctx) -> new QueuedMessageCounts(operation.queueName,
                                                                                                                                                                              rs.getLong("regular_count"),
                                                                                                                                                                              rs.getLong("dead_letter_count")))
                                                                                                                                    .one()))
                .proceed();
    }

    @Override
    public final long getTotalDeadLetterMessagesQueuedFor(GetTotalDeadLetterMessagesQueuedFor operation) {
        requireNonNull(operation, "You must specify a GetTotalDeadLetterMessagesQueuedFor instance");
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> unitOfWorkFactory.withUnitOfWork(handleAwareUnitOfWork -> handleAwareUnitOfWork.handle().createQuery(durableQueuesSql.getGetTotalDeadLetterMessagesQueuedForSql())
                                                                                                                                    .bind("queueName", operation.queueName)
                                                                                                                                    .mapTo(Long.class)
                                                                                                                                    .one()))
                .proceed();
    }

    @Override
    public final int purgeQueue(PurgeQueue operation) {
        requireNonNull(operation, "You must specify a PurgeQueue instance");
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> unitOfWorkFactory.withUnitOfWork(handleAwareUnitOfWork -> handleAwareUnitOfWork.handle().createUpdate(durableQueuesSql.getPurgeQueueSql())
                                                                                                                                    .bind("queueName", operation.queueName)
                                                                                                                                    .execute()))
                .proceed();
    }

    @Override
    public final List<NextQueuedMessage> queryForMessagesSoonReadyForDelivery(QueueName queueName, Instant withNextDeliveryTimestampAfter, int maxNumberOfMessagesToReturn) {
        requireNonNull(queueName, "No queueName provided");
        requireNonNull(withNextDeliveryTimestampAfter, "No withNextDeliveryTimestampAfter provided");


        return unitOfWorkFactory.withUnitOfWork(handleAwareUnitOfWork -> handleAwareUnitOfWork.handle().createQuery(durableQueuesSql.getQueryForMessagesSoonReadyForDeliverySql())
                                                                                              .bind("queueName", requireNonNull(queueName, "No QueueName provided"))
                                                                                              .bind("now", withNextDeliveryTimestampAfter)
                                                                                              .bind("pageSize", maxNumberOfMessagesToReturn)
                                                                                              .map((rs, ctx) -> new NextQueuedMessage(QueueEntryId.of(rs.getString("id")),
                                                                                                                                      queueName,
                                                                                                                                      rs.getObject("added_ts", OffsetDateTime.class).toInstant(),
                                                                                                                                      rs.getObject("next_delivery_ts", OffsetDateTime.class).toInstant()))
                                                                                              .list());
    }

    @Override
    public final List<QueuedMessage> getQueuedMessages(GetQueuedMessages operation) {
        requireNonNull(operation, "You must specify a GetQueuedMessages instance");
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> queryQueuedMessages(operation.queueName, operation.getQueueingSortOrder(), IncludeMessages.QUEUED_MESSAGES, operation.getStartIndex(), operation.getPageSize()))
                .proceed();
    }

    @Override
    public final List<QueuedMessage> getDeadLetterMessages(GetDeadLetterMessages operation) {
        requireNonNull(operation, "You must specify a GetDeadLetterMessages instance");
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> queryQueuedMessages(operation.queueName, operation.getQueueingSortOrder(), IncludeMessages.DEAD_LETTER_MESSAGES, operation.getStartIndex(), operation.getPageSize()))
                .proceed();
    }


    protected enum IncludeMessages {
        ALL, DEAD_LETTER_MESSAGES, QUEUED_MESSAGES
    }

    List<QueuedMessage> queryQueuedMessages(QueueName queueName, QueueingSortOrder queueingSortOrder, IncludeMessages includeMessages, long startIndex, long pageSize) {
        requireNonNull(queueName, "No queueName provided");
        requireNonNull(queueingSortOrder, "No queueingOrder provided");
        requireNonNull(includeMessages, "No includeMessages provided");

        Supplier<String> resolveIncludeMessagesSql = () -> {
            switch (includeMessages) {
                case ALL:
                    return "";
                case DEAD_LETTER_MESSAGES:
                    return "AND is_dead_letter_message = TRUE\n";
                case QUEUED_MESSAGES:
                    return "AND is_dead_letter_message = FALSE\n";
                default:
                    throw new IllegalArgumentException("Unsupported IncludeMessages value: " + includeMessages);
            }
        };

        return unitOfWorkFactory.withUnitOfWork(handleAwareUnitOfWork -> handleAwareUnitOfWork.handle().createQuery(bind("SELECT * FROM {:tableName} \n" +
                                                                                                                                 " WHERE queue_name = :queueName\n" +
                                                                                                                                 "{:includeMessages}" +
                                                                                                                                 " LIMIT :pageSize \n" +
                                                                                                                                 " OFFSET :offset",
                                                                                                                         arg("tableName", sharedQueueTableName),
                                                                                                                         arg("includeMessages", resolveIncludeMessagesSql.get())))
                                                                                              .bind("queueName", requireNonNull(queueName, "No QueueName provided"))
                                                                                              .bind("offset", startIndex)
                                                                                              .bind("pageSize", pageSize)
                                                                                              .map(queuedMessageMapper)
                                                                                              .list());
    }

    @Override
    public DurableQueues addInterceptor(DurableQueuesInterceptor interceptor) {
        requireNonNull(interceptor, "No interceptor provided");
        log.info("Adding interceptor: {}", interceptor);
        interceptor.setDurableQueues(this);
        interceptors.add(interceptor);
        sortInterceptorsByOrder(interceptors);
        return this;
    }

    @Override
    public DurableQueues removeInterceptor(DurableQueuesInterceptor interceptor) {
        requireNonNull(interceptor, "No interceptor provided");
        log.info("Removing interceptor: {}", interceptor);
        interceptors.remove(interceptor);
        sortInterceptorsByOrder(interceptors);
        return this;
    }

    @Override
    public Optional<QueueName> getQueueNameFor(QueueEntryId queueEntryId) {
        return unitOfWorkFactory.withUnitOfWork(handleAwareUnitOfWork -> handleAwareUnitOfWork.handle()
                                                                                              .createQuery(bind("SELECT queue_name FROM {:tableName} WHERE \n" +
                                                                                                                        " id = :id",
                                                                                                                arg("tableName", sharedQueueTableName)))
                                                                                              .bind("id", requireNonNull(queueEntryId, "No queueEntryId provided"))
                                                                                              .mapTo(QueueName.class)
                                                                                              .findOne());
    }

    @Override
    public Optional<QueuedMessage> getDeadLetterMessage(GetDeadLetterMessage operation) {
        requireNonNull(operation, "You must specify a GetDeadLetterMessage instance");
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> getQueuedMessage(operation.queueEntryId, true))
                .proceed();
    }

    /**
     * Implementation of the batch message fetching capability to optimize database queries
     * by fetching messages across multiple queues in a single operation.
     *
     * @param queueNames                   the queue names to fetch messages for
     * @param excludeKeysPerQueue          map of queue name to set of keys to exclude (for ordered messages)
     * @param availableWorkerSlotsPerQueue map of queue name to number of available worker slots
     * @return list of queued messages ready for delivery
     */
    @Override
    public List<QueuedMessage> fetchNextBatchOfMessages(Collection<QueueName> queueNames,
                                                        Map<QueueName, Set<String>> excludeKeysPerQueue,
                                                        Map<QueueName, Integer> availableWorkerSlotsPerQueue) {
        requireNonNull(queueNames, "No queueNames provided");
        requireNonNull(excludeKeysPerQueue, "No excludeKeysPerQueue provided");
        requireNonNull(availableWorkerSlotsPerQueue, "No availableWorkerSlotsPerQueue provided");

        return fetchNextBatchOfMessages(queueNames, excludeKeysPerQueue, availableWorkerSlotsPerQueue, useOrderedUnorderedQuery);
    }

    public List<QueuedMessage> fetchNextBatchOfMessages(Collection<QueueName> queueNames,
                                                        Map<QueueName, Set<String>> excludeKeysPerQueue,
                                                        Map<QueueName, Integer> availableWorkerSlotsPerQueue,
                                                        boolean useOrderedUnorderedQuery) {
        log.trace("Fetching batch of messages for queues: {}", queueNames);
        if (queueNames.isEmpty()) {
            return Collections.emptyList();
        }

        try {
            return unitOfWorkFactory.withUnitOfWork(uow -> {
                resetMessagesStuckBeingDeliveredAcrossMultipleQueues(queueNames);

                var now         = Instant.now();
                var allMessages = new ArrayList<QueuedMessage>();

                for (var queueName : queueNames) {
                    var availableWorkerSlotsForThisQueue = availableWorkerSlotsPerQueue.get(queueName);
                    if (availableWorkerSlotsForThisQueue == null || availableWorkerSlotsForThisQueue <= 0) {
                        log.trace("[{}] Skipping queue as it has no available worker slots", queueName);
                        continue;
                    }
                    var consumer = durableQueueConsumers.get(queueName);
                    if (consumer == null) {
                        log.trace("[{}] Skipping queue as it has no consumer", queueName);
                        continue;
                    }
                    var optimizer = consumer.getQueuePollingOptimizer();
                    if (optimizer.shouldSkipPolling()) {
                        log.trace("[{}] skipping centralized polling", queueName);
                        continue;
                    }

                    var                 excluded = excludeKeysPerQueue.getOrDefault(queueName, Collections.emptySet());
                    List<QueuedMessage> messagesForQueue;
                    MessageMappingResult mappingResult;

                    if (useOrderedUnorderedQuery) {
                        var orderedSql = durableQueuesSql.buildOrderedSqlStatement(!excluded.isEmpty());
                        var orderedQ = uow.handle().createQuery(orderedSql)
                                          .bind("queueName", queueName)
                                          .bind("now", now)
                                          .bind("limit", availableWorkerSlotsForThisQueue);
                        if (!excluded.isEmpty()) orderedQ.bindList("excludeKeys", excluded);

                        mappingResult = mapQueryResultsWithExceptionHandling(orderedQ);
                        messagesForQueue = mappingResult.successfulMessages();
                        handleFailedMappings(queueName, mappingResult);

                        if (messagesForQueue.isEmpty()) {
                            var unorderedSql = durableQueuesSql.buildUnorderedSqlStatement();
                            var unorderedQ = uow.handle().createQuery(unorderedSql)
                                                .bind("queueName", queueName)
                                                .bind("now", now)
                                                .bind("limit", availableWorkerSlotsForThisQueue);
                            mappingResult = mapQueryResultsWithExceptionHandling(unorderedQ);
                            messagesForQueue = mappingResult.successfulMessages();
                            handleFailedMappings(queueName, mappingResult);
                        }
                    } else {
                        var sql = durableQueuesSql.buildGetNextMessageReadyForDeliverySqlStatement(excluded);
                        var query = uow.handle().createQuery(sql)
                                       .bind("queueName", queueName)
                                       .bind("now", now)
                                       .bind("limit", availableWorkerSlotsForThisQueue);
                        if (!excluded.isEmpty()) query.bindList("excludedKeys", new ArrayList<>(excluded));
                        mappingResult = mapQueryResultsWithExceptionHandling(query);
                        messagesForQueue = mappingResult.successfulMessages();
                        handleFailedMappings(queueName, mappingResult);
                    }

                    log.debug("[{}] Batch fetched {} messages with {} slots available",
                              queueName, messagesForQueue.size(), availableWorkerSlotsForThisQueue);
                    if (messagesForQueue.isEmpty()) {
                        optimizer.queuePollingReturnedNoMessages();
                        log.trace("[{}] No messages fetched for this queue", queueName);
                    } else {
                        optimizer.queuePollingReturnedMessages(messagesForQueue);
                        log.trace("[{}] Fetched {} messages for this queue", queueName, messagesForQueue.size());
                    }
                    allMessages.addAll(messagesForQueue);
                }

                log.debug("Batch fetched {} messages for {} queues: {}",
                          allMessages.size(), queueNames.size(), queueNames);
                return allMessages;
            });
        } catch (Exception e) {
            if (IOExceptionUtil.isIOException(e)) {
                log.debug("Error in fetchNextBatchOfMessages: {}", e.getMessage(), e);
            } else {
                log.error("Error in fetchNextBatchOfMessages: {}", e.getMessage(), e);
            }
            return Collections.emptyList();
        }
    }

    /**
     * Work in progress - doesn't handle competing consumers yet
     */
    @Override
    public List<QueuedMessage> fetchNextBatchOfMessagesBatched(Collection<QueueName> queueNames,
                                                               Map<QueueName, Set<String>> excludeKeysPerQueue,
                                                               Map<QueueName, Integer> availableWorkerSlotsPerQueue) {

        log.trace("Fetching batch of messages for queues: {}", queueNames);

        // 1) Filter out queues with no slots or that should skip polling
        List<QueueName> activeQueues = queueNames.stream()
                                                 .filter(queueName -> availableWorkerSlotsPerQueue.getOrDefault(queueName, 0) > 0)
                                                 .filter(queueName -> {
                                                     var durableQueueConsumer = durableQueueConsumers.get(queueName);
                                                     if (durableQueueConsumer == null) {
                                                         log.trace("[{}] Skipping queue as it has no consumer", queueName);
                                                         return false;
                                                     }
                                                     var     queuePollingOptimizer = durableQueueConsumer.getQueuePollingOptimizer();
                                                     boolean skip                  = queuePollingOptimizer.shouldSkipPolling();
                                                     if (skip) log.trace("[{}] skipping due to backoff", queueName);
                                                     return !skip;
                                                 })
                                                 .toList();

        if (activeQueues.isEmpty()) {
            return Collections.emptyList();
        }

        try {
            return unitOfWorkFactory.withUnitOfWork(uow -> {
                resetMessagesStuckBeingDeliveredAcrossMultipleQueues(activeQueues);

                var now = Instant.now();
                var batchedSqlResult = durableQueuesSql.buildBatchedSqlStatement(excludeKeysPerQueue, availableWorkerSlotsPerQueue, activeQueues);
                var query = uow.handle()
                               .createQuery(batchedSqlResult.getSql())
                               .bind("now", now);
                
                // Bind single-value parameters (queue names)
                for (var entry : batchedSqlResult.getSingleValueBindings().entrySet()) {
                    query.bind(entry.getKey(), entry.getValue());
                }
                
                // Bind list parameters (exclude keys)
                for (var entry : batchedSqlResult.getListBindings().entrySet()) {
                    query.bindList(entry.getKey(), entry.getValue());
                }
                
                var mappingResult = mapQueryResultsWithExceptionHandling(query);
                var messages = mappingResult.successfulMessages();
                
                // Log failed mappings for monitoring purposes
                if (!mappingResult.failedMappings().isEmpty()) {
                    log.warn("Failed to deserialize {} messages during batch fetch. Failed QueueEntryIds: {}",
                             mappingResult.failedMappings().size(),
                             mappingResult.failedMappings().stream()
                                          .map(failed -> failed.queueEntryId().toString())
                                          .collect(Collectors.joining(", ")));
                    
                    // Log individual failures for debugging
                    for (var failedMapping : mappingResult.failedMappings()) {
                        log.error("[{}] Marking Message as DeadLetterMessage due to issues "
                                          + "while deserializing message with id '{}'",
                                  failedMapping.queueName(), failedMapping.queueEntryId(), failedMapping.mappingException());
                        markAsDeadLetterMessage(failedMapping.queueEntryId(), failedMapping.mappingException());
                    }
                }

                Map<QueueName, List<QueuedMessage>> byQueue = messages.stream()
                                                                      .collect(Collectors.groupingBy(QueuedMessage::getQueueName));

                for (var queueName : activeQueues) {
                    var durableQueueConsumer = durableQueueConsumers.get(queueName);
                    if (durableQueueConsumer == null) {
                        log.trace("[{}] Skipping queue as it has no consumer", queueName);
                        continue;
                    }
                    var                 queuePollingOptimizer = durableQueueConsumer.getQueuePollingOptimizer();
                    List<QueuedMessage> messagesForQueue      = byQueue.getOrDefault(queueName, Collections.emptyList());
                    if (messagesForQueue.isEmpty()) {
                        queuePollingOptimizer.queuePollingReturnedNoMessages();
                    } else {
                        queuePollingOptimizer.queuePollingReturnedMessages(messagesForQueue);
                    }
                }

                log.debug("Batch fetched {} messages across {} queues: {}",
                          messages.size(),
                          activeQueues.size(),
                          byQueue.entrySet().stream()
                                 .collect(Collectors.toMap(Map.Entry::getKey, e -> e.getValue().size())));

                return messages;
            });
        } catch (Exception e) {
            if (IOExceptionUtil.isIOException(e)) {
                log.debug("Error in fetchNextBatchOfMessages: {}", e.getMessage(), e);
            } else {
                log.error("Error in fetchNextBatchOfMessages: {}", e.getMessage(), e);
            }
            return Collections.emptyList();
        }
    }

    /**
     * Maps query results to QueuedMessage objects while handling JSONDeserializationException.
     * Messages that fail to deserialize are captured with their QueueEntryId and exception.
     *
     * @param query the JDBI query to execute and map
     * @return MessageMappingResult containing successful messages and failed mappings
     */
    private MessageMappingResult mapQueryResultsWithExceptionHandling(org.jdbi.v3.core.statement.Query query) {
        List<QueuedMessage> successfulMessages = new ArrayList<>();
        List<MessageMappingResult.FailedMessageMapping> failedMappings = new ArrayList<>();

        // Use a custom row mapper that handles exceptions
        var customMapper = new RowMapper<QueuedMessage>() {
            @Override
            public QueuedMessage map(ResultSet rs, StatementContext ctx) throws java.sql.SQLException {
                try {
                    // Use the existing mapper to process the row
                    return queuedMessageMapper.map(rs, ctx);
                } catch (Exception e) {
                    QueueName queueName = QueueName.of(rs.getString("queue_name"));
                    QueueEntryId queueEntryId = QueueEntryId.of(rs.getString("id"));
                    failedMappings.add(new MessageMappingResult.FailedMessageMapping(queueName, queueEntryId, e));
                    return null;
                }
            }
        };

        // Execute the query and filter out null results
        List<QueuedMessage> allResults = query.map(customMapper).list();
        successfulMessages.addAll(allResults.stream().filter(Objects::nonNull).toList());

        return new MessageMappingResult(successfulMessages, failedMappings);
    }

    /**
     * Handles failed message mappings by logging and marking them as dead letter messages.
     * Uses a direct SQL update to avoid re-deserializing the payload (which would fail again).
     *
     * @param queueName     the queue name for logging context
     * @param mappingResult the result containing failed mappings to handle
     */
    private void handleFailedMappings(QueueName queueName, MessageMappingResult mappingResult) {
        for (var failedMapping : mappingResult.failedMappings()) {
            log.error("[{}] Marking Message as DeadLetterMessage due to deserialization failure for message id '{}'",
                      queueName, failedMapping.queueEntryId(), failedMapping.mappingException());
            try {
                // Use markAsDeadLetterMessageDirect() instead of markAsDeadLetterMessage() because:
                // 1. markAsDeadLetterMessage() returns the updated message using the regular mapper
                // 2. The regular mapper would try to deserialize the payload again
                // 3. This would fail with the same deserialization error, causing an infinite loop
                var success = markAsDeadLetterMessageDirect(failedMapping.queueEntryId(), failedMapping.mappingException());
                if (success) {
                    log.debug("[{}] Successfully marked message '{}' as dead letter due to deserialization failure",
                              queueName, failedMapping.queueEntryId());
                } else {
                    log.warn("[{}] Failed to mark message '{}' as dead letter - message may have been deleted",
                             queueName, failedMapping.queueEntryId());
                }
            } catch (Exception e) {
                log.error("[{}] Error marking message '{}' as dead letter: {}",
                          queueName, failedMapping.queueEntryId(), e.getMessage(), e);
            }
        }
    }

    /**
     * Reset stuck messages that are marked as being delivered but haven't been acknowledged
     * across multiple queues at once
     *
     * @param queueNames the queue names to check for stuck messages
     */
    private void resetMessagesStuckBeingDeliveredAcrossMultipleQueues(Collection<QueueName> queueNames) {
        requireNonNull(queueNames, "No queueNames provided");
        if (transactionalMode != TransactionalMode.SingleOperationTransaction || queueNames.isEmpty()) {
            return;
        }

        log.trace("resetMultipleQueuesStuckBeingDelivered called for queues: {}", queueNames);
        var now = Instant.now();
        var queuesToReset = queueNames.stream()
                                      .filter(queueName -> {
                                          var lastReset = lastResetStuckMessagesCheckTimestamps.get(queueName);
                                          return lastReset == null ||
                                                  Duration.between(now, lastReset).abs().toMillis() > messageHandlingTimeoutMs;
                                      })
                                      .collect(Collectors.toList());

        if (queuesToReset.isEmpty()) {
            log.trace("No stuck messages to reset across multiple queues: {}", queueNames);
            return;
        }

        log.debug("Looking for messages stuck marked as isBeingDelivered across queues: {}", queuesToReset);

        var queueNamesForQuery = queuesToReset.stream()
                                              .map(QueueName::toString)
                                              .toList();

        var numberOfChanges = unitOfWorkFactory.getRequiredUnitOfWork().handle().createUpdate(durableQueuesSql.getResetMessagesStuckBeingDeliveredAcrossMultipleQueuesSql())
                                               .bind("threshold", now.minusMillis(messageHandlingTimeoutMs))
                                               .bind("error", "Handler Processing of the Message was determined to have Timed Out")
                                               .bind("now", now)
                                               .bindList("queueNames", queueNamesForQuery)
                                               .execute();

        if (numberOfChanges > 0) {
            log.debug("Reset {} messages stuck marked as isBeingDelivered across queues: {}",
                      numberOfChanges,
                      queuesToReset);
        } else {
            log.debug("No stuck messages found across queues: {}", queuesToReset);
        }

        // Update timestamps for all queues we checked
        queuesToReset.forEach(queueName -> lastResetStuckMessagesCheckTimestamps.put(queueName, now));
    }

    @Override
    public final Optional<QueuedMessage> getQueuedMessage(GetQueuedMessage operation) {
        requireNonNull(operation, "You must specify a GetQueuedMessage instance");
        return newInterceptorChainForOperation(operation,
                                               interceptors,
                                               (interceptor, interceptorChain) -> interceptor.intercept(operation, interceptorChain),
                                               () -> getQueuedMessage(operation.queueEntryId, false))
                .proceed();
    }

    private Optional<QueuedMessage> getQueuedMessage(QueueEntryId queueEntryId, boolean isDeadLetterMessage) {
        return unitOfWorkFactory.withUnitOfWork(handleAwareUnitOfWork -> handleAwareUnitOfWork.handle().createQuery(bind("SELECT * FROM {:tableName} WHERE \n" +
                                                                                                                                 " id = :id AND\n" +
                                                                                                                                 " is_dead_letter_message = :isDeadLetterMessage",
                                                                                                                         arg("tableName", sharedQueueTableName)))
                                                                                              .bind("id", requireNonNull(queueEntryId, "No queueEntryId provided"))
                                                                                              .bind("isDeadLetterMessage", isDeadLetterMessage)
                                                                                              .map(queuedMessageMapper)
                                                                                              .findOne());
    }

}
